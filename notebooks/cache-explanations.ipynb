{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eddf750f",
   "metadata": {},
   "source": [
    "# Generate explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ca9f2",
   "metadata": {},
   "source": [
    "In this notebook, we generate explanations for all samples of an example dataset and cache it for visualization in SemLa.\n",
    "\n",
    "This notebook depends on the outputs from `embed-dataset.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc480b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from curses import raw\n",
    "from lib2to3.pgen2 import token\n",
    "from flask import Flask, send_from_directory, request\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from transformers import BertModel\n",
    "from transformers.models.bert.modeling_bert import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "\n",
    "MAX_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd86829",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "DATASET = \"banking\"\n",
    "if DATASET == \"banking\":\n",
    "    num_labels = 77\n",
    "\n",
    "@jit\n",
    "def inner_product_distance(a,b, tau=15):\n",
    "    return np.exp(-np.sum(a * b) / tau)**2\n",
    "\n",
    "\n",
    "class BertForImportanceAttribution(BertModel):\n",
    "\n",
    "    def setMode(self, mode):\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                precomputed_encoding=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                output_hidden_states=False,\n",
    "                output_attentions=False):\n",
    "\n",
    "        if self.mode == \"integrad_from_similarity\":\n",
    "            encoding = super().forward(input_ids).last_hidden_state[:,0]\n",
    "            \n",
    "            if precomputed_encoding is not None:\n",
    "                import torch\n",
    "\n",
    "                similarity = torch.inner(encoding, precomputed_encoding)\n",
    "                similarity = similarity.sum(dim=-1)\n",
    "                return similarity\n",
    "            else:\n",
    "                return encoding\n",
    "\n",
    "        elif self.mode == \"integrad\":\n",
    "            encoding = super().forward(input_ids).last_hidden_state[:,0]\n",
    "            output = encoding.sum(dim=-1)\n",
    "            return output\n",
    "        \n",
    "        elif self.mode == \"vanilla_grad\":\n",
    "            extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_ids.size(), device=DEVICE)\n",
    "            embedding_output = self.embeddings(\n",
    "                input_ids=input_ids,\n",
    "                position_ids=None,\n",
    "                token_type_ids=token_type_ids,\n",
    "                inputs_embeds=None,\n",
    "                past_key_values_length=0,\n",
    "            )\n",
    "\n",
    "        \n",
    "            embedding_output.retain_grad()\n",
    "\n",
    "            encoder_outputs = self.encoder(\n",
    "                embedding_output,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                head_mask=None,\n",
    "                encoder_hidden_states=None,\n",
    "                encoder_attention_mask=None,\n",
    "                past_key_values=None,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=output_hidden_states\n",
    "            )\n",
    "            sequence_output = encoder_outputs[0]\n",
    "            pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "            return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "                last_hidden_state=sequence_output,\n",
    "                pooler_output=pooled_output,\n",
    "            ), embedding_output\n",
    "        elif self.mode is None:\n",
    "            return super().forward(input_ids, \n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    output_attentions=output_attentions\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a7a87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_with_matching_case(tokens, text):\n",
    "    to_return = []\n",
    "    for token in tokens:\n",
    "        text_lowercase = text.lower()\n",
    "        \n",
    "        token = token.lower()\n",
    "        pos = text_lowercase.find(token)\n",
    "        to_return.append(text[pos:len(token)])\n",
    "        text = text[len(token):].strip()\n",
    "        \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c319df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_importance(tokenizer, model, text, device=\"cuda\"):\n",
    "    with torch.no_grad():\n",
    "        tokenized_input = tokenizer(text,\n",
    "                                    max_length=MAX_LENGTH, \n",
    "                                    truncation=True, \n",
    "                                    return_tensors=\"pt\")\n",
    "        tokenized_input.to(device)\n",
    "        model.to(device)\n",
    "        outputs = model(**tokenized_input, output_attentions=True)\n",
    "        attentions = torch.stack(outputs.attentions)\n",
    "        attentions_aggregated = attentions.squeeze().sum(0).sum(0).detach().cpu()\n",
    "        attentions_importance = attentions_aggregated.sum(0) / 144\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0])\n",
    "        tokens = tokens[1:-1]\n",
    "\n",
    "        importance = attentions_importance.tolist()[1:-1]\n",
    "    return importance, tokens\n",
    "\n",
    "\n",
    "def lime_importance(tokenizer, model, text, support_set, device=None):\n",
    "\n",
    "    def one_sentence_tokenize(text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token.replace(\"#\", \"\") for token in tokens]\n",
    "        tokens = get_tokens_with_matching_case(tokens, text)\n",
    "        return tokens\n",
    "\n",
    "    def encode(texts):\n",
    "        with torch.no_grad():\n",
    "            tokenized_xs = tokenizer.batch_encode_plus(texts, max_length=50, \n",
    "                                                    truncation=True, padding=True, return_tensors=\"pt\")\n",
    "            outputs = []\n",
    "            num_batches = math.ceil(len(texts) / BATCH_LIMIT)\n",
    "            \n",
    "            for batch_idx in range(num_batches):\n",
    "                tokenized_xs_batched = dict(\n",
    "                    input_ids=tokenized_xs[\"input_ids\"][batch_idx*BATCH_LIMIT: (batch_idx + 1) * BATCH_LIMIT].to(device),\n",
    "                    token_type_ids=tokenized_xs[\"token_type_ids\"][batch_idx*BATCH_LIMIT: (batch_idx + 1) * BATCH_LIMIT].to(device),\n",
    "                    attention_mask=tokenized_xs[\"attention_mask\"][batch_idx*BATCH_LIMIT: (batch_idx + 1) * BATCH_LIMIT].to(device),\n",
    "                )\n",
    "                outputs_batched = model(**tokenized_xs_batched)\n",
    "                outputs.append( outputs_batched.last_hidden_state[:,0,:] )\n",
    "        \n",
    "            outputs = torch.cat(outputs)\n",
    "            return outputs\n",
    "\n",
    "    def classify(texts):\n",
    "        import torch\n",
    "        text_encoding = encode(texts)\n",
    "        similarities = torch.inner(text_encoding, support_encodings)\n",
    "        probs = torch.softmax(similarities/TAU, dim=-1)\n",
    "        return probs.detach().cpu().numpy()\n",
    "\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if len(text.split(\" \")) < 10 else \"cpu\"\n",
    "\n",
    "    model.to(device)\n",
    "    BATCH_LIMIT = 1\n",
    "    TAU = 15\n",
    "\n",
    "    support_encodings = encode(support_set[\"text\"])\n",
    "    probs = classify([text])\n",
    "    label = probs[0].argmax()\n",
    "\n",
    "    LIME_explainer_1sent = LimeTextExplainer(\n",
    "                        class_names=support_set[\"label\"], \n",
    "                        bow=False, \n",
    "                        split_expression=one_sentence_tokenize, \n",
    "                        mask_string=tokenizer.mask_token)\n",
    "    exp = LIME_explainer_1sent.explain_instance(text,\n",
    "                        classify, top_labels=5, num_samples=100, \n",
    "                        num_features=MAX_LENGTH)\n",
    "    \n",
    "    tokens = one_sentence_tokenize(text)\n",
    "    importance = sorted(exp.as_map()[label], key=lambda x: x[0])\n",
    "    importance = [weight for pos, weight in importance]\n",
    "    return importance, tokens\n",
    "\n",
    "\n",
    "def integrad_importance(tokenizer, model, text, txt2=None, device=\"cuda\"):\n",
    "    from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "    model.to(device)\n",
    "    model.setMode(\"integrad_from_similarity\")\n",
    "    with torch.no_grad():\n",
    "        tokenized_inputs2 = tokenizer(txt2, \n",
    "                                max_length=MAX_LENGTH,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "        tokenized_inputs2.to(device)\n",
    "        encoding2 = model(tokenized_inputs2[\"input_ids\"])\n",
    "\n",
    "    lig = LayerIntegratedGradients(model, model.embeddings)\n",
    "    tokenized_inputs = tokenizer(text, \n",
    "                                max_length=MAX_LENGTH, \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\")\n",
    "    tokenized_inputs.to(device)\n",
    "    input = tokenized_inputs[\"input_ids\"]\n",
    "    \n",
    "    attributions_ig, delta = lig.attribute(\n",
    "        (input, encoding2), \n",
    "        return_convergence_delta=True,\n",
    "        attribute_to_layer_input=False\n",
    "    )\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    importance = attributions_ig.sum(-1).squeeze().tolist()[1:-1]\n",
    "    model.setMode(None)\n",
    "\n",
    "    return importance, tokens\n",
    "\n",
    "\n",
    "def gradient_importance(tokenizer, model, text, txt2=None, device=\"cuda\"):\n",
    "    def encode(text, tokenizer, model, device=\"cuda\", output_last_hiddens=False):\n",
    "        tokenized_inputs = tokenizer(text, \n",
    "                                    max_length=MAX_LENGTH, \n",
    "                                    truncation=True, \n",
    "                                    return_tensors=\"pt\")\n",
    "        tokenized_inputs.to(device)\n",
    "        model.to(device)\n",
    "        \n",
    "        outputs, embeddings = model(**tokenized_inputs,\n",
    "                            output_hidden_states=True)\n",
    "        encoding = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "        if output_last_hiddens:\n",
    "            return encoding, embeddings, outputs.last_hidden_state.squeeze()[1:-1,:]\n",
    "        return encoding, embeddings\n",
    "\n",
    "    model.to(device)\n",
    "    model.setMode(None)\n",
    "    with torch.no_grad():\n",
    "        tokenized_inputs2 = tokenizer(txt2, \n",
    "                                max_length=MAX_LENGTH,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "        tokenized_inputs2.to(device)\n",
    "        encoding2 = model(tokenized_inputs2[\"input_ids\"]).last_hidden_state[:,0]\n",
    "\n",
    "    model.setMode(\"vanilla_grad\")\n",
    "    \n",
    "    encoding, embeddings = encode(text, tokenizer, model, device)\n",
    "    similarity = torch.inner(encoding, encoding2)\n",
    "    similarity = similarity.sum(dim=-1).backward()\n",
    "    \n",
    "    importance = embeddings.grad.abs().sum(-1).squeeze().tolist()[1:-1]\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    model.setMode(None)\n",
    "    return importance, tokens\n",
    "    \n",
    "\n",
    "def token_encoding_relation(tokenizer, model, txt1, txt2, device=\"cuda\"):\n",
    "    def encode(text, tokenizer, model, device=\"cuda\"):\n",
    "        tokenized_inputs = tokenizer(text, \n",
    "                                    max_length=MAX_LENGTH, \n",
    "                                    truncation=True, \n",
    "                                    return_tensors=\"pt\")\n",
    "        tokenized_inputs.to(device)\n",
    "        model.to(device)\n",
    "        \n",
    "        outputs = model(**tokenized_inputs,\n",
    "                            output_hidden_states=True)\n",
    "        return outputs.last_hidden_state.squeeze()[1:-1,:]\n",
    "    \n",
    "    encodings1 = encode(txt1, tokenizer, model, device)\n",
    "    encodings2 = encode(txt2, tokenizer, model, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_similarities = torch.inner(encodings1, encodings2)\n",
    "    \n",
    "    return {\"links\": token_similarities.tolist(),\n",
    "            \"tokens1\": tokenizer.tokenize(txt1),\n",
    "            \"tokens2\": tokenizer.tokenize(txt2)}\n",
    "\n",
    "\n",
    "def integrad_relation(tokenizer, model, txt1, txt2, device=\"cuda\"):\n",
    "    from captum.attr import LayerIntegratedGradients\n",
    "\n",
    "    model.setMode(\"integrad_from_similarity\")\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        tokenized_inputs2 = tokenizer(txt2, \n",
    "                                max_length=MAX_LENGTH,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "        tokenized_inputs2.to(device)\n",
    "        encoding2 = model(tokenized_inputs2[\"input_ids\"])\n",
    "\n",
    "    lig = LayerIntegratedGradients(model, model.embeddings)\n",
    "    tokenized_inputs = tokenizer(txt1, \n",
    "                                    max_length=MAX_LENGTH,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "    tokenized_inputs.to(device)\n",
    "    input = tokenized_inputs[\"input_ids\"]\n",
    "\n",
    "    attributions_ig, delta = lig.attribute(\n",
    "        (input, encoding2), \n",
    "        return_convergence_delta=True,\n",
    "        attribute_to_layer_input=False\n",
    "    )\n",
    "    tokens = tokenizer.tokenize(txt1)\n",
    "    importance = attributions_ig.sum(-1).squeeze().tolist()[1:-1]\n",
    "    model.setMode(None)\n",
    "\n",
    "    return tokens, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0c01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, dataset, num_labels):\n",
    "        self.tau = 15\n",
    "\n",
    "        # Load dataset for its metadata\n",
    "        from datasets import Dataset, DatasetDict\n",
    "        from sklearn import preprocessing\n",
    "\n",
    "        self.datasets = dict()\n",
    "        self.prediction_data = dict()\n",
    "\n",
    "        for dataset in [\"banking77\"]:\n",
    "            dataset = \"banking77\"\n",
    "            raw_datasets = load_dataset(dataset)            \n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(raw_datasets[\"train\"][\"label\"] + raw_datasets[\"test\"][\"label\"])\n",
    "            self.datasets[dataset] = raw_datasets\n",
    "\n",
    "            import json\n",
    "            prediction_data_file = \"./results/banking77-with-BERT-Banking77/banking77-viz_data-12-clusters-label_cluster_chosen_by_majority_in-predicted-label-with-BERT-Banking77.json\"\n",
    "            with open(prediction_data_file, \"r\") as f:\n",
    "                self.prediction_data[dataset] = json.load(f)\n",
    "\n",
    "        # Load model\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "        checkpoint = \"philschmid/BERT-Banking77\" \n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = BertForImportanceAttribution.from_pretrained(checkpoint)\n",
    "        self.model.setMode(None)\n",
    "        self.model.to(DEVICE)\n",
    "\n",
    "    def process(self, text):\n",
    "        encoding = self.encode(text)\n",
    "        return {\"encoding\": encoding.tolist()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokenized = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        tokenized.to(DEVICE)\n",
    "        outputs = self.model(**tokenized)\n",
    "        return outputs.last_hidden_state.squeeze()[0].detach().cpu().numpy()\n",
    "    \n",
    "    def importance(self, dataset, index, method):\n",
    "        text = self.datasets[dataset][\"test\"][\"text\"][index]\n",
    "\n",
    "        pred_data = self.prediction_data[dataset][index]\n",
    "        support_set_idxs = pred_data[\"support_set\"]\n",
    "        support_set = self.datasets[dataset][\"test\"][support_set_idxs]\n",
    "        distances = np.array(pred_data[\"distances\"]).squeeze()\n",
    "        closest_dp_idx = distances.argmax()\n",
    "        closest_text = support_set[\"text\"][closest_dp_idx]\n",
    "\n",
    "        if method == \"attention\":\n",
    "            importance, tokens = attention_importance(self.tokenizer, self.model, text)\n",
    "            return importance, tokens\n",
    "        elif method == \"lime\":\n",
    "            importance = lime_importance(self.tokenizer, self.model, text, support_set)\n",
    "            return importance\n",
    "        elif method == \"integrad\":\n",
    "            importance = integrad_importance(self.tokenizer, self.model, text, txt2=closest_text)\n",
    "            return importance\n",
    "        elif method == \"gradient\":\n",
    "            importance = gradient_importance(self.tokenizer, self.model, text, txt2=closest_text)\n",
    "            return importance\n",
    "\n",
    "    def importances_all(self, dataset, index):\n",
    "        attn_importance, tokens = self.importance(dataset, index, \"attention\")\n",
    "        lime_importance, tokens = self.importance(dataset, index, \"lime\")\n",
    "        integrad_importance, tokens = self.importance(dataset, index, \"integrad\")\n",
    "        grad_importance, tokens = self.importance(dataset, index, \"gradient\")\n",
    "\n",
    "        return {\"tokens\": tokens, \n",
    "                \"attn_importance\": attn_importance,\n",
    "                \"lime_importance\": lime_importance,\n",
    "                \"grad_importance\": grad_importance, \n",
    "                \"integrad_importance\": integrad_importance}\n",
    "    \n",
    "    def relation(self, dataset, index1, index2, reltype):\n",
    "        txt1 = self.datasets[dataset][\"test\"][\"text\"][index1]\n",
    "        txt2 = self.datasets[dataset][\"test\"][\"text\"][index2]\n",
    "\n",
    "        if reltype == \"token2token\":\n",
    "            return token_encoding_relation(\n",
    "                self.tokenizer,\n",
    "                self.model,\n",
    "                txt1,\n",
    "                txt2,\n",
    "            )\n",
    "        else:\n",
    "            tokens1, importance1 = integrad_relation(\n",
    "                self.tokenizer,\n",
    "                self.model,\n",
    "                txt1,\n",
    "                txt2\n",
    "            )\n",
    "            tokens2, importance2 = integrad_relation(\n",
    "                self.tokenizer,\n",
    "                self.model,\n",
    "                txt2,\n",
    "                txt1\n",
    "            )\n",
    "            return {\"tokens1\": tokens1,\n",
    "                    \"tokens2\": tokens2,\n",
    "                    \"importance1\": importance1,\n",
    "                    \"importance2\": importance2,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e86e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/mojo/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n",
      "Some weights of the model checkpoint at philschmid/BERT-Banking77 were not used when initializing BertForImportanceAttribution: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertForImportanceAttribution from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForImportanceAttribution from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextProcessor(DATASET, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf96342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/mojo/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn import preprocessing\n",
    "\n",
    "all_datasets = dict()\n",
    "prediction_data = dict()\n",
    "for dataset in [\"banking77\"]:\n",
    "    raw_datasets = load_dataset(dataset)            \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(raw_datasets[\"train\"][\"label\"] + raw_datasets[\"test\"][\"label\"])\n",
    "\n",
    "    all_datasets[dataset] = raw_datasets\n",
    "\n",
    "    import json\n",
    "    prediction_data_file = \"results/banking77-with-BERT-Banking77/banking77-viz_data-12-clusters-label_cluster_chosen_by_majority_in-predicted-label-with-BERT-Banking77.json\"\n",
    "    with open(prediction_data_file, \"r\") as f:\n",
    "        prediction_data[dataset] = json.load(f)\n",
    "        \n",
    "    prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6049fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contrastive_idxs(entry, data):\n",
    "    support_set = entry[\"support_set\"]\n",
    "    support_set_distances = entry[\"distances\"]\n",
    "    support_set_topk = torch.topk(torch.tensor(support_set_distances), len(support_set))\n",
    "    support_set_sorted = [support_set[idx] \n",
    "                              for idx in support_set_topk.indices.squeeze().tolist()]\n",
    "    support_set_labels = [data[idx][\"ground_truth_label_idx\"] for idx in support_set]\n",
    "    fact_idx = support_set_sorted[0]\n",
    "\n",
    "    is_error = entry[\"prediction_label_idx\"] != entry[\"ground_truth_label_idx\"]\n",
    "    if is_error:\n",
    "        # if it's an error, the contrast instance is the ground_truth instance\n",
    "        support_set_labels = [data[idx][\"ground_truth_label_idx\"] for idx in support_set]\n",
    "        same_label_support = [i for i, label in enumerate(support_set_labels) \n",
    "                             if label == entry[\"ground_truth_label_idx\"]][0]\n",
    "        contrast_idx = support_set[same_label_support]\n",
    "    else:\n",
    "        # if it's not an error, the contrast instance is the second closest\n",
    "        contrast_idx = support_set_sorted[1]\n",
    "    return fact_idx, contrast_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f74d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "\n",
    "demo_data_dir = \"./results/banking77-with-BERT-Banking77/explanations\"\n",
    "\n",
    "if not os.path.isdir(demo_data_dir):\n",
    "    os.mkdir(demo_data_dir)\n",
    "\n",
    "for dataset_name, data in prediction_data.items():\n",
    "    if dataset_name in [\"clinc\", \"banking\"]:\n",
    "        continue\n",
    "    print(dataset_name, len(data))\n",
    "    importances = []\n",
    "    tok2tok_relations = []\n",
    "    tok2sim_relations = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        entry = data[i]\n",
    "        fact_idx, contrast_idx = get_contrastive_idxs(entry, data)\n",
    "        is_error = entry[\"prediction_label_idx\"] != entry[\"ground_truth_label_idx\"]\n",
    "        \n",
    "        print(i, fact_idx, contrast_idx, \"is_error:\", is_error, \"|\", entry[\"text\"], )\n",
    "        print(\"\\t\\t\\t\", data[fact_idx][\"text\"])\n",
    "        print(\"\\t\\t\\t\", data[contrast_idx][\"text\"])\n",
    "        \n",
    "        importance_all = text_processor.importances_all(dataset_name, i)\n",
    "        tok2tok_rel_left = text_processor.relation(\n",
    "                                        dataset_name, \n",
    "                                        i, \n",
    "                                        contrast_idx,\n",
    "                                        \"token2token\")\n",
    "        tok2tok_rel_right = text_processor.relation(\n",
    "                                        dataset_name,\n",
    "                                        i, \n",
    "                                        fact_idx,\n",
    "                                        \"token2token\")        \n",
    "        tok2sim_rel_left = text_processor.relation(\n",
    "                                        dataset_name, \n",
    "                                        i, \n",
    "                                        contrast_idx, \n",
    "                                        \"token2sim\")\n",
    "        tok2sim_rel_right = text_processor.relation(\n",
    "                                        dataset_name, \n",
    "                                        i, \n",
    "                                        fact_idx, \n",
    "                                        \"token2sim\")\n",
    "        importances.append(importance_all)\n",
    "        tok2tok_relations.append(dict(left=tok2tok_rel_left, \n",
    "                                      right=tok2tok_rel_right))\n",
    "        tok2sim_relations.append(dict(left=tok2sim_rel_left,\n",
    "                                      right=tok2sim_rel_right))\n",
    "        print(tok2tok_rel_left[\"tokens1\"], tok2tok_rel_left[\"tokens2\"])\n",
    "        print(tok2sim_rel_left[\"tokens1\"], tok2sim_rel_right[\"tokens2\"])\n",
    "        \n",
    "        if (i % 500 == 0):\n",
    "            with open(f\"{demo_data_dir}/importances.json\", \"w\") as f:\n",
    "                json.dump(importances, f)\n",
    "            with open(f\"{demo_data_dir}/token2token_relations.json\", \"w\") as f:\n",
    "                json.dump(tok2tok_relations, f)\n",
    "            with open(f\"{demo_data_dir}/token2similarity_relations.json\", \"w\") as f:\n",
    "                json.dump(tok2sim_relations, f)\n",
    "            with open(f\"{demo_data_dir}/num_examples.json\", \"w\") as f:\n",
    "                json.dump({\"count\": i}, f)\n",
    "    \n",
    "    with open(f\"{demo_data_dir}/importances.json\", \"w\") as f:\n",
    "        json.dump(importances, f)\n",
    "    with open(f\"{demo_data_dir}/token2token_relations.json\", \"w\") as f:\n",
    "        json.dump(tok2tok_relations, f)\n",
    "    with open(f\"{demo_data_dir}/token2similarity_relations.json\", \"w\") as f:\n",
    "        json.dump(tok2sim_relations, f)\n",
    "    with open(f\"{demo_data_dir}/num_examples.json\", \"w\") as f:\n",
    "        json.dump({\"count\": i}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
